{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import faulthandler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from vctr.models.lstm.defaults import SEQUENCE_LENGTH\n",
    "from vctr.models.lstm.lstm_model import HybridModel\n",
    "from vctr.models.lstm.main import VCNet\n",
    "from vctr.models.lstm.utils import get_new_model_name\n",
    "from vctr.models.lstm.actions import new_model, save_model, train_model, load_model\n",
    "from vctr.trading.coins import trainable_coins\n",
    "\n",
    "faulthandler.enable()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "num_classes = 3\n",
    "sequence_length = SEQUENCE_LENGTH\n",
    "\n",
    "# model = HybridModel(\n",
    "#     num_features=379,\n",
    "#     num_classes=3,\n",
    "#     hidden_size=128,\n",
    "#     out_channels=256,\n",
    "#     num_layers=2,\n",
    "#     kernel_size=3,\n",
    "#     use_conv=True,\n",
    "#     bidirectional=True,\n",
    "# )\n",
    "model = VCNet(input_size=320, hidden_size=256, num_classes=3, num_layers=2)\n",
    "model.name = get_new_model_name()\n",
    "# model_name = 'lstm-mk-189'\n",
    "# model = load_model('lstm-mk-649-COTI-XTZ-LOOM-LRC-BNT')\n",
    "\n",
    "trn_params = []\n",
    "\n",
    "coins = random.sample(trainable_coins, 5)\n",
    "\n",
    "for coin in ['REN']:\n",
    "    clear_output()\n",
    "    model.name = f'{model.name}-{coin}'\n",
    "    print(f'Training on {coin}')\n",
    "    print(f'Current model: {model.name}')\n",
    "    train_model(\n",
    "        model=model,\n",
    "        epochs=200,\n",
    "        symbol=coin,\n",
    "        timeframes=['15m'],\n",
    "        batch_size=512,\n",
    "        learning_rate=0.00001,\n",
    "        label_args=(0.03, 0.005),\n",
    "        test_pct=0.3,\n",
    "        sequence_length=sequence_length,\n",
    "        find_optimal_lr=False,\n",
    "    )\n",
    "\n",
    "    save_model(model, model.name)\n",
    "    save_model(model, 'latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import faulthandler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from vctr.models.lstm.defaults import SEQUENCE_LENGTH\n",
    "from vctr.models.lstm.utils import get_new_model_name\n",
    "from vctr.models.lstm.actions import save_model\n",
    "from vctr.models.lstm.genetic import run_genetic_algorithm\n",
    "from vctr.trading.coins import trainable_coins\n",
    "from vctr.models.lstm.main import VCNet\n",
    "from vctr.models.lstm.lstm_model import HybridModel\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "faulthandler.enable()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "test_pct = 0.3\n",
    "\n",
    "label_args = (0.05, 0.005)\n",
    "cxpb = 0.5\n",
    "mutpb = 0.3\n",
    "population_size = 100\n",
    "\n",
    "\n",
    "def model_evaluation(hidden_size, out_channels, num_layers, kernel_size, bidirectional):\n",
    "    hidden_size = int(hidden_size)\n",
    "    out_channels = int(out_channels)\n",
    "    num_layers = int(num_layers)\n",
    "    kernel_size = int(kernel_size)\n",
    "    bidirectional = bool(round(bidirectional))\n",
    "\n",
    "    model = HybridModel(\n",
    "        num_features=486,\n",
    "        num_classes=3,\n",
    "        bidirectional=bidirectional,\n",
    "        hidden_size=hidden_size,\n",
    "        out_channels=out_channels,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "    )\n",
    "    model.name = get_new_model_name()\n",
    "\n",
    "    coins = random.sample(trainable_coins, 5)\n",
    "\n",
    "    for coin in ['RNDR']:  # ['REN']:\n",
    "        clear_output()\n",
    "        model.name = f'{model.name}-{coin}'\n",
    "        print(f'Training on {coin}')\n",
    "        print(f'Current model: {model.name}')\n",
    "        fitness_score = run_genetic_algorithm(\n",
    "            model=model,\n",
    "            symbol=coin,\n",
    "            timeframes=['30m'],\n",
    "            batch_size=512,\n",
    "            sequence_length=SEQUENCE_LENGTH,\n",
    "            label_args=label_args,\n",
    "            test_pct=test_pct,\n",
    "            cxpb=cxpb,\n",
    "            population_size=population_size,\n",
    "            mutpb=mutpb,\n",
    "            ngen=5,  # Change to 5 generations\n",
    "            # start='2021-01-01',\n",
    "            # end='2021-02-01',\n",
    "        )\n",
    "\n",
    "        save_model(model, model.name)\n",
    "        save_model(model, 'latest')\n",
    "\n",
    "    return fitness_score\n",
    "\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "pbounds = {\n",
    "    'hidden_size': (4, 512),\n",
    "    'out_channels': (4, 128),\n",
    "    'num_layers': (1, 4),\n",
    "    'kernel_size': (1, 5),\n",
    "    'bidirectional': (0, 1),\n",
    "}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=model_evaluation,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Optimize the hyperparameters\n",
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = optimizer.max['params']\n",
    "\n",
    "# Convert the float values of the best_params dictionary to their appropriate types\n",
    "best_params['hidden_size'] = int(best_params['hidden_size'])\n",
    "best_params['out_channels'] = int(best_params['out_channels'])\n",
    "best_params['num_layers'] = int(best_params['num_layers'])\n",
    "best_params['kernel_size'] = int(best_params['kernel_size'])\n",
    "best_params['bidirectional'] = bool(round(best_params['bidirectional']))\n",
    "\n",
    "print('Best hyperparameters found:', best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on TRX\n",
      "Current model: lstm-mk-1293-TRX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import faulthandler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from vctr.models.lstm.defaults import SEQUENCE_LENGTH\n",
    "from vctr.models.lstm.utils import get_new_model_name\n",
    "from vctr.models.lstm.actions import save_model, load_model\n",
    "from vctr.models.lstm.genetic import run_genetic_algorithm\n",
    "from vctr.trading.coins import trainable_coins\n",
    "from vctr.models.lstm.lstm_model import HybridModel\n",
    "from vctr.models.lstm.main import VCNet\n",
    "\n",
    "faulthandler.enable()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "test_pct = 0.3\n",
    "\n",
    "# @ChatGPT Find the best hyperparameters for the genetic algorithm\n",
    "cxpb = 0.7\n",
    "mutpb = 0.6\n",
    "population_size = 100\n",
    "ngen = 50\n",
    "label_args = (0.07, 0.01)\n",
    "hidden_size = 64\n",
    "out_channels = 51\n",
    "num_layers = 5\n",
    "kernel_size = 3\n",
    "bidirectional = True\n",
    "use_conv = True\n",
    "dropout = 0.2\n",
    "\n",
    "model = HybridModel(\n",
    "    num_features=297,\n",
    "    num_classes=3,\n",
    "    bidirectional=bidirectional,\n",
    "    hidden_size=hidden_size,\n",
    "    out_channels=out_channels,\n",
    "    num_layers=num_layers,\n",
    "    kernel_size=kernel_size,\n",
    "    use_conv=use_conv,\n",
    ")\n",
    "# model = VCNet(input_size=379, hidden_size=8, num_classes=3, num_layers=2)\n",
    "\n",
    "# model = load_model('latest')\n",
    "model.name = get_new_model_name()\n",
    "\n",
    "coins = random.sample(trainable_coins, 5)\n",
    "\n",
    "for coin in ['TRX']:\n",
    "    clear_output()\n",
    "    model.name = f'{model.name}-{coin}'\n",
    "    print(f'Training on {coin}')\n",
    "    print(f'Current model: {model.name}')\n",
    "    run_genetic_algorithm(\n",
    "        model=model,\n",
    "        symbol=coin,\n",
    "        timeframes=['30m'],\n",
    "        batch_size=256,\n",
    "        sequence_length=SEQUENCE_LENGTH,\n",
    "        label_args=label_args,\n",
    "        test_pct=test_pct,\n",
    "        cxpb=cxpb,\n",
    "        population_size=population_size,\n",
    "        mutpb=mutpb,\n",
    "        ngen=ngen,\n",
    "    )\n",
    "\n",
    "    save_model(model, model.name)\n",
    "    save_model(model, 'latest')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vctr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
