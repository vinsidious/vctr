{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from vctr.models.lstm.data import get_train_and_val_loaders, get_train_and_val_loaders_with_close\n",
    "\n",
    "from vctr.models.lstm.defaults import SEQUENCE_LENGTH\n",
    "from vctr.utils.label_tracker import LabelTracker\n",
    "from vctr.trading.coins import tradable_coins\n",
    "from vctr.utils.pf_stats import get_plot_and_pf, print_stats\n",
    "from vctr.models.lstm.actions import load_model, dynamic_threshold_adjustment\n",
    "from vctr.data.lstm_preprocessor import preprocess_data\n",
    "from vctr.data.data_loader import get_data_with_features_and_labels\n",
    "\n",
    "import faulthandler\n",
    "\n",
    "faulthandler.enable()\n",
    "\n",
    "model = load_model('latest')\n",
    "# model = load_model('lstm-mk-943-REN-XTZ')\n",
    "# model = load_model('lstm-mk-820-BTC-highest-profit')\n",
    "\n",
    "timeframes = ['15m']\n",
    "start = datetime.now() - timedelta(days=180)\n",
    "end = datetime.now() - timedelta(days=90)\n",
    "label_args = (0.035, 0.005)\n",
    "batch_size = 1\n",
    "\n",
    "window_size = SEQUENCE_LENGTH\n",
    "sequence_length = SEQUENCE_LENGTH\n",
    "\n",
    "\n",
    "class WindowedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, window_size):\n",
    "        self.X = torch.tensor(X).float().to('mps')\n",
    "        self.y = torch.tensor(y).long().to('mps')\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_slice = self.X[index : index + self.window_size]\n",
    "        y_slice = self.y[index : index + self.window_size]\n",
    "        return X_slice, y_slice\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.window_size + 1\n",
    "\n",
    "\n",
    "returns = []\n",
    "tracker = LabelTracker('win_rates.json')\n",
    "\n",
    "for symbol in tradable_coins:\n",
    "    # df = get_data_with_features_and_labels(\n",
    "    #     symbol=symbol,\n",
    "    #     timeframes=timeframes,\n",
    "    #     label_args=label_args,\n",
    "    #     start=start,\n",
    "    #     # end=end,\n",
    "    #     separate=False,\n",
    "    # )\n",
    "\n",
    "    train_loader, val_loader, data = get_train_and_val_loaders(\n",
    "        start=start,\n",
    "        end=end,\n",
    "        symbol=symbol,\n",
    "        timeframes=timeframes,\n",
    "        lookback=sequence_length,\n",
    "        label_args=label_args,\n",
    "        batch_size=batch_size,\n",
    "        test_pct=0,\n",
    "        return_data=True,\n",
    "    )\n",
    "\n",
    "    # Make `data` the same length as the training data\n",
    "    df = data[-len(train_loader.dataset) :]\n",
    "\n",
    "    # X_cls = df.drop(['label'], axis=1)\n",
    "    # X_cls = StandardScaler().fit_transform(X_cls)\n",
    "\n",
    "    # X, y = preprocess_data(df.drop(['label'], axis=1), df['label'], lookback=sequence_length, torch=True)\n",
    "\n",
    "    # dataset = WindowedDataset(X, y, window_size)\n",
    "    # loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # all_logits = []\n",
    "    # for inputs, targets in tqdm(loader, colour='green', ncols=65):\n",
    "    #     with torch.no_grad():\n",
    "    #         logits = model(inputs.to('mps')[0])\n",
    "    #         all_logits.append(logits[-1:])\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to('mps'), target.to('mps')\n",
    "            logits = model(data)\n",
    "            all_logits.append(logits)\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_predictions = torch.argmax(all_logits, dim=1).cpu().numpy()\n",
    "    all_predictions = dynamic_threshold_adjustment(all_logits, [0.96, 0.02, 0.02])\n",
    "\n",
    "    df = df[-len(all_predictions) :].copy()\n",
    "    df['pred'] = pd.Series(all_predictions, index=df.index)\n",
    "\n",
    "    fig, pf = get_plot_and_pf(df, 'pred', pf_params={'sl_stop': None}, width=1050, height=600)\n",
    "\n",
    "    fig.data[0].line.width = 1\n",
    "    fig.data[0].opacity = 0.75\n",
    "    fig.show()\n",
    "\n",
    "    print(f'Symbol: {symbol}')\n",
    "    print_stats(pf)\n",
    "    returns.append(pf.total_return)\n",
    "    tracker.track(symbol, pf.sortino_ratio)\n",
    "    print(f'Average return: {np.mean(returns) * 100:.0f}%')\n",
    "\n",
    "print(model.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vctr.models.lstm.actions import save_model\n",
    "\n",
    "\n",
    "save_model(model, f'{model.name}-saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vctr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
